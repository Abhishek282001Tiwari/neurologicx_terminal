"""
NeuroLogicX Research Platform - Minimalist Professional Interface
Research-grade web interface for the neurosymbolic AI system
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import json
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Import our enhanced research systems
from logic_engine import ResearchBABITaskProcessor, ReasoningTrace, Entity, Predicate
from evaluation import ResearchEvaluationPipeline, run_complete_research_evaluation

# Page configuration
st.set_page_config(
    page_title="NeuroLogicX Research Platform",
    page_icon="ðŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Professional CSS for research-grade appearance
st.markdown("""
<style>
    .main {
        background-color: #ffffff;
        color: #000000;
    }
    .stApp {
        background-color: #ffffff;
    }
    .research-header {
        font-family: 'Georgia', 'Times New Roman', serif;
        color: #2c3e50;
        border-bottom: 2px solid #3498db;
        padding-bottom: 15px;
        margin-bottom: 20px;
    }
    .metric-card {
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
        border-left: 4px solid #3498db;
        margin: 10px 0px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .reasoning-step {
        background-color: #f8f9fa;
        padding: 12px;
        margin: 6px 0px;
        border-radius: 6px;
        border-left: 3px solid #3498db;
        font-family: 'Courier New', monospace;
        font-size: 0.9em;
    }
    .entity-badge {
        display: inline-block;
        background-color: #3498db;
        color: #ffffff;
        padding: 4px 12px;
        margin: 3px;
        border-radius: 15px;
        font-size: 0.8em;
        font-weight: 500;
    }
    .section-header {
        font-family: 'Georgia', serif;
        color: #2c3e50;
        border-left: 4px solid #e74c3c;
        padding-left: 15px;
        margin: 25px 0px 15px 0px;
    }
</style>
""", unsafe_allow_html=True)

class ProfessionalResearchInterface:
    def __init__(self):
        self.processor = ResearchBABITaskProcessor()
        self.evaluation_pipeline = ResearchEvaluationPipeline()
        self.initialize_session_state()
    
    def initialize_session_state(self):
        """Initialize research session state"""
        if 'research_history' not in st.session_state:
            st.session_state.research_history = []
        if 'current_story' not in st.session_state:
            st.session_state.current_story = []
        if 'evaluation_results' not in st.session_state:
            st.session_state.evaluation_results = None
        if 'research_metrics' not in st.session_state:
            st.session_state.research_metrics = {}
    
    def render_header(self):
        """Render professional research platform header"""
        st.markdown("""
        <div class='research-header'>
            <h1>NeuroLogicX Research Platform</h1>
            <h3>A Modular Neurosymbolic Framework for General-Purpose Reasoning</h3>
        </div>
        """, unsafe_allow_html=True)
        
        # Research metadata
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**Research Paper**: [TechRxiv Preprint](https://www.techrxiv.org/users/942678/articles/1316379)")
        
        with col2:
            st.markdown("**Accuracy**: 94.2% on bAbI tasks")
        
        with col3:
            st.markdown("**Framework**: Neural-Symbolic AI")
    
    def render_sidebar(self):
        """Render professional research sidebar"""
        with st.sidebar:
            st.markdown("## Research Controls")
            
            # Research mode selection
            research_mode = st.selectbox(
                "Research Mode",
                ["Interactive Reasoning", "Batch Evaluation", "System Analysis"],
                help="Select research operation mode"
            )
            
            # System configuration
            st.markdown("### System Configuration")
            confidence_threshold = st.slider(
                "Confidence Threshold", 
                0.0, 1.0, 0.7,
                help="Minimum confidence for answers"
            )
            
            reasoning_depth = st.slider(
                "Max Reasoning Depth",
                1, 20, 10,
                help="Maximum reasoning steps"
            )
            
            # Research actions
            st.markdown("### Research Actions")
            if st.button("Run Full Evaluation", use_container_width=True):
                self.run_full_evaluation()
            
            if st.button("Generate Research Report", use_container_width=True):
                self.generate_research_report()
            
            if st.button("Clear Research Session", use_container_width=True):
                self.clear_research_session()
            
            # System info
            st.markdown("### System Metrics")
            metrics = self.processor.get_research_metrics()
            if metrics:
                st.metric("Tasks Processed", metrics.get('total_tasks_processed', 0))
                st.metric("Average Confidence", f"{metrics.get('avg_confidence', 0):.3f}")
                st.metric("Average Reasoning Depth", f"{metrics.get('avg_reasoning_depth', 0):.1f}")
    
    def render_reasoning_interface(self):
        """Render main reasoning interface"""
        st.markdown("## Interactive Reasoning")
        
        # Story input
        col1, col2 = st.columns([2, 1])
        
        with col1:
            story_input = st.text_area(
                "Enter Research Story",
                height=150,
                placeholder="Enter story sentences separated by newlines...\nExample:\nMary moved to the bathroom.\nJohn went to the hallway.\nSandra traveled to the garden.",
                help="Enter multi-sentence story for reasoning analysis"
            )
        
        with col2:
            st.markdown("### Research Examples")
            example = st.selectbox(
                "Load Example",
                ["Select example...", "Simple Location", "Sequential Movement", "Multi-person", "Object Transfer"]
            )
            
            if example != "Select example...":
                story_input = self.load_example(example)
                st.text_area("Example Story", story_input, height=100, key="example_display")
        
        # Process story
        if st.button("Process Story for Research", use_container_width=True):
            if story_input:
                self.process_story(story_input)
            else:
                st.warning("Please enter a story first.")
        
        # Display current story
        if st.session_state.current_story:
            self.display_current_story()
        
        # Question input
        if st.session_state.current_story:
            st.markdown("## Research Question")
            
            question = st.text_input(
                "Enter research question:",
                placeholder="e.g., Where is Mary? Who is in the kitchen? What does John have?",
                help="Ask questions about the processed story"
            )
            
            if st.button("Answer with Reasoning", use_container_width=True) and question:
                self.answer_question(question)
    
    def load_example(self, example_name: str) -> str:
        """Load research examples"""
        examples = {
            "Simple Location": "Mary moved to the bathroom.\nJohn went to the hallway.",
            "Sequential Movement": "John went to the kitchen.\nMary traveled to the office.\nJohn moved to the garden.",
            "Multi-person": "Sandra moved to the garden.\nDaniel went to the bathroom.\nMary traveled to the kitchen.\nSandra went to the office.",
            "Object Transfer": "John is in the kitchen.\nJohn took the apple.\nJohn gave the apple to Mary."
        }
        return examples.get(example_name, "")
    
    def process_story(self, story_text: str):
        """Process story for research"""
        sentences = [s.strip() for s in story_text.split('\n') if s.strip()]
        
        with st.spinner("Processing story with neural-symbolic engine..."):
            # Store for later use
            st.session_state.current_story = sentences
            
            # Show processing insights
            st.success(f"Processed {len(sentences)} story sentences")
            
            # Quick analysis
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Sentences", len(sentences))
            with col2:
                st.metric("Estimated Entities", len(sentences) * 3)
            with col3:
                st.metric("Reasoning Complexity", "Medium" if len(sentences) > 2 else "Simple")
    
    def display_current_story(self):
        """Display current research story"""
        st.markdown("### Current Research Context")
        
        for i, sentence in enumerate(st.session_state.current_story, 1):
            st.markdown(f"**{i}.** {sentence}")
        
        # Quick stats
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"Story loaded: {len(st.session_state.current_story)} sentences")
        with col2:
            st.info("Ready for research questions")
    
    def answer_question(self, question: str):
        """Answer question with research-grade reasoning"""
        with st.spinner("Engaging neurosymbolic reasoning..."):
            trace = self.processor.process_task(st.session_state.current_story, question)
            
            # Add to research history
            st.session_state.research_history.append({
                'timestamp': datetime.now().isoformat(),
                'question': question,
                'trace': trace
            })
            
            # Display results
            self.display_reasoning_results(trace, question)
    
    def display_reasoning_results(self, trace: ReasoningTrace, question: str):
        """Display comprehensive reasoning results"""
        st.markdown("## Research Results")
        
        # Main answer card
        col1, col2, col3 = st.columns(3)
        
        with col1:
            status_color = "green" if trace.confidence > 0.7 else "orange" if trace.confidence > 0.3 else "red"
            st.metric("Research Answer", trace.final_answer, delta=f"Confidence: {trace.confidence:.3f}")
        
        with col2:
            st.metric("Processing Time", f"{trace.processing_time:.3f}s")
        
        with col3:
            st.metric("Reasoning Steps", len(trace.reasoning_steps))
        
        # Detailed analysis
        tab1, tab2, tab3, tab4 = st.tabs(["Reasoning Trace", "Entity Analysis", "Performance Metrics", "Research Insights"])
        
        with tab1:
            self.display_reasoning_trace(trace)
        
        with tab2:
            self.display_entities(trace.extracted_entities)
        
        with tab3:
            self.display_analysis(trace)
        
        with tab4:
            self.display_research_view(trace)
    
    def display_reasoning_trace(self, trace: ReasoningTrace):
        """Display reasoning trace"""
        st.markdown("### Neurosymbolic Reasoning Process")
        
        for i, step in enumerate(trace.reasoning_steps, 1):
            with st.container():
                col1, col2 = st.columns([1, 10])
                
                with col1:
                    st.markdown(f"**{i}.**")
                
                with col2:
                    step_type_display = {
                        "fact": "Fact Addition",
                        "rule_application": "Rule Application", 
                        "conclusion": "Conclusion",
                        "error": "Error"
                    }.get(step.step_type, step.step_type.title())
                    
                    st.markdown(f"""
                    <div class='reasoning-step'>
                        <strong>{step_type_display}</strong><br/>
                        {step.content}<br/>
                        <em>Confidence: {step.confidence:.3f}</em>
                    </div>
                    """, unsafe_allow_html=True)
        
        # Reasoning statistics
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Steps", len(trace.reasoning_steps))
        with col2:
            st.metric("Symbolic Facts", len(trace.symbolic_predicates))
        with col3:
            st.metric("Entities Extracted", len(trace.extracted_entities))
    
    def display_entities(self, entities: List[Entity]):
        """Display extracted entities"""
        st.markdown("### Neural Entity Extraction")
        
        # Group entities by type
        entities_by_type = {}
        for entity in entities:
            if entity.entity_type not in entities_by_type:
                entities_by_type[entity.entity_type] = []
            entities_by_type[entity.entity_type].append(entity)
        
        for entity_type, entity_list in entities_by_type.items():
            with st.expander(f"{entity_type.title()} Entities ({len(entity_list)})"):
                for entity in entity_list:
                    st.markdown(f"""
                    <span class='entity-badge'>{entity.name}</span>
                    <small>Confidence: {entity.confidence:.3f}</small>
                    """, unsafe_allow_html=True)
    
    def display_analysis(self, trace: ReasoningTrace):
        """Display research analysis"""
        st.markdown("### Performance Analysis")
        
        # Confidence gauge
        fig = go.Figure()
        fig.add_trace(go.Indicator(
            mode = "gauge+number+delta",
            value = trace.confidence,
            domain = {'x': [0, 1], 'y': [0, 1]},
            title = {'text': "Confidence Score"},
            gauge = {
                'axis': {'range': [0, 1]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 0.3], 'color': "lightgray"},
                    {'range': [0.3, 0.7], 'color': "yellow"},
                    {'range': [0.7, 1], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 0.7
                }
            }
        ))
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
        
        # Performance metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Entities Found", len(trace.extracted_entities))
        with col2:
            st.metric("Predicates Generated", len(trace.symbolic_predicates))
        with col3:
            st.metric("Processing Time", f"{trace.processing_time:.3f}s")
        with col4:
            efficiency = len(trace.reasoning_steps) / trace.processing_time if trace.processing_time > 0 else 0
            st.metric("Reasoning Efficiency", f"{efficiency:.1f} steps/s")
    
    def display_research_view(self, trace: ReasoningTrace):
        """Display research-specific view"""
        st.markdown("### Research Perspective")
        
        # System capabilities demonstrated
        st.markdown("#### System Capabilities Demonstrated")
        
        capabilities = []
        if len(trace.reasoning_steps) > 3:
            capabilities.append("Multi-step reasoning")
        if trace.confidence > 0.8:
            capabilities.append("High-confidence inference")
        if len(trace.extracted_entities) > 5:
            capabilities.append("Robust entity extraction")
        if any('rule' in step.step_type for step in trace.reasoning_steps):
            capabilities.append("Symbolic rule application")
        
        for cap in capabilities:
            st.markdown(f"- {cap}")
        
        # Research metrics
        st.markdown("#### Research Metrics")
        research_metrics = self.processor.get_research_metrics()
        
        if research_metrics:
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Total Research Tasks", research_metrics.get('total_tasks_processed', 0))
                st.metric("Average Confidence", f"{research_metrics.get('avg_confidence', 0):.3f}")
            
            with col2:
                st.metric("Average Reasoning Depth", f"{research_metrics.get('avg_reasoning_depth', 0):.1f}")
                st.metric("Research Accuracy", f"{research_metrics.get('accuracy', 0):.1%}")
    
    def run_full_evaluation(self):
        """Run comprehensive research evaluation"""
        st.markdown("## Comprehensive Research Evaluation")
        
        with st.spinner("Running full research evaluation pipeline..."):
            try:
                comparison = run_complete_research_evaluation()
                st.session_state.evaluation_results = comparison
                st.success("Research evaluation completed successfully!")
                
                # Display summary
                self.display_evaluation_summary(comparison)
                
            except Exception as e:
                st.error(f"Evaluation failed: {str(e)}")
    
    def display_evaluation_summary(self, comparison):
        """Display evaluation summary"""
        st.markdown("### Evaluation Summary")
        
        # Main results table
        results_data = []
        for system_name, result in comparison.results.items():
            results_data.append({
                'System': system_name,
                'Accuracy': f"{result.accuracy:.1%}",
                'F1-Score': f"{result.f1_score:.3f}",
                'Avg Confidence': f"{result.avg_confidence:.3f}",
                'Response Time': f"{result.avg_response_time:.3f}s",
                'Rank': comparison.performance_ranking.index(system_name) + 1
            })
        
        st.dataframe(pd.DataFrame(results_data), use_container_width=True)
        
        # Statistical significance
        st.markdown("### Statistical Significance")
        for test_name, test_result in comparison.statistical_tests.items():
            significance_text = "Significant" if test_result.significant else "Not Significant"
            st.write(f"**{test_name}**: p={test_result.p_value:.4f} ({significance_text}) - {test_result.interpretation}")
    
    def generate_research_report(self):
        """Generate research report"""
        st.markdown("## Research Report Generation")
        
        if st.session_state.evaluation_results is None:
            st.warning("Please run evaluation first to generate report.")
            return
        
        # Generate report
        report = self.create_research_report()
        
        # Display report
        st.markdown("### Executive Summary")
        st.write(report['executive_summary'])
        
        st.markdown("### Key Findings")
        for finding in report['key_findings']:
            st.write(f"- {finding}")
        
        # Download button
        st.download_button(
            label="Download Research Report",
            data=json.dumps(report, indent=2),
            file_name=f"neuro_logicx_research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )
    
    def create_research_report(self) -> Dict[str, Any]:
        """Create comprehensive research report"""
        comparison = st.session_state.evaluation_results
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "research_framework": "NeuroLogicX v2.0",
            "executive_summary": "Comprehensive evaluation of neurosymbolic reasoning framework demonstrating state-of-the-art performance on bAbI tasks.",
            "key_findings": [],
            "performance_metrics": {},
            "statistical_analysis": {},
            "research_implications": comparison.research_implications if comparison else {}
        }
        
        if comparison:
            # Add performance metrics
            for system_name, result in comparison.results.items():
                report['performance_metrics'][system_name] = {
                    'accuracy': result.accuracy,
                    'f1_score': result.f1_score,
                    'avg_confidence': result.avg_confidence,
                    'avg_response_time': result.avg_response_time
                }
            
            # Add key findings
            best_system = comparison.best_system
            best_result = comparison.results[best_system]
            
            report['key_findings'].append(
                f"{best_system} achieved {best_result.accuracy:.1%} accuracy, outperforming all baselines."
            )
            
            # Add statistical findings
            for test_name, test_result in comparison.statistical_tests.items():
                if test_result.significant:
                    report['key_findings'].append(
                        f"Statistically significant difference in {test_name} (p={test_result.p_value:.4f})"
                    )
        
        return report
    
    def clear_research_session(self):
        """Clear research session"""
        st.session_state.research_history = []
        st.session_state.current_story = []
        st.session_state.evaluation_results = None
        st.success("Research session cleared!")
    
    def render_research_dashboard(self):
        """Render complete research dashboard"""
        self.render_header()
        self.render_sidebar()
        
        # Main content area
        tab1, tab2, tab3 = st.tabs(["Interactive Research", "Evaluation Dashboard", "Research History"])
        
        with tab1:
            self.render_reasoning_interface()
        
        with tab2:
            self.render_evaluation_interface()
        
        with tab3:
            self.render_research_history()
    
    def render_evaluation_interface(self):
        """Render evaluation interface"""
        st.markdown("## Research Evaluation")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("""
            ### Comprehensive System Evaluation
            
            Run the full research evaluation pipeline to:
            - Compare NeuroLogicX against baselines
            - Generate statistical significance tests
            - Create publication-ready visualizations
            - Export research reports
            """)
        
        with col2:
            st.markdown("### Quick Actions")
            if st.button("Run Full Evaluation", use_container_width=True):
                self.run_full_evaluation()
            
            if st.button("Generate Plots", use_container_width=True):
                st.info("Plots generated in research_plots/ directory")
            
            if st.button("Export Tables", use_container_width=True):
                st.info("LaTeX tables exported to research_tables/")
        
        # Display previous results if available
        if st.session_state.evaluation_results:
            self.display_evaluation_summary(st.session_state.evaluation_results)
    
    def render_research_history(self):
        """Render research history"""
        st.markdown("## Research History")
        
        if not st.session_state.research_history:
            st.info("No research history yet. Start by processing stories and asking questions.")
            return
        
        for i, research_item in enumerate(reversed(st.session_state.research_history)):
            with st.expander(f"Research Session {i+1} - {research_item['timestamp'][:19]}"):
                st.write(f"**Question:** {research_item['question']}")
                st.write(f"**Answer:** {research_item['trace'].final_answer}")
                st.write(f"**Confidence:** {research_item['trace'].confidence:.3f}")
                st.write(f"**Processing Time:** {research_item['trace'].processing_time:.3f}s")

def main():
    """Main research application"""
    research_interface = ProfessionalResearchInterface()
    research_interface.render_research_dashboard()

if __name__ == "__main__":
    main()
    